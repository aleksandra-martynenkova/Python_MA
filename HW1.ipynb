{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b8bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import logging\n",
    "import nltk.data\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa7624",
   "metadata": {},
   "source": [
    "First of all, preprocessing of some piece from 'Crime and Punishment' by Dostoevsky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c201dded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\almar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "file=open(\"/Users/almar/OneDrive/Рабочий стол/python/prestuplenie_i_nakazanie1.txt\", mode='r+')\n",
    "r = file.read()\n",
    "\n",
    "r_low = r.lower() #lowering\n",
    "no_punc = re.sub(r'[^\\w\\s]','',r_low) #no punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f1e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f47c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = no_punc.splitlines()\n",
    "pin_lines = []\n",
    "for l in lines:\n",
    "    lemmas = m.lemmatize(l)\n",
    "    new_line = ''.join(lemmas)\n",
    "    pin_lines.append(new_line) #split by lines and lemmatize\n",
    "pin2 ='\\n'.join(pin_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5de676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open(\"pin_lem.txt\", \"w+\")\n",
    "my_file.write(pin2)\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6691a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5171c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/Users/almar/pin_lem.txt\", mode = 'r')\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b69ba",
   "metadata": {},
   "source": [
    "Initially, I started with the following parameteres: size = 100, window = 5, min_count = 2, epochs = 10. It seems that the most suitable combination for the corpus is 300, 5, 2, 35. Size less than 300 results in unwanted output of prepositions and pronouns rather than meaningful words (especially for wv.most_similar). Window size of 5 remained default since increase of window size (e.g. 8-10) leads to unwanted results as well. It appears that the higher number of interation (in this case 35), the higher chance to exclude unwanted words like prepositions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "27175426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%time model_pin = gensim.models.Word2Vec(data, vector_size=300, window=5, min_count=2, epochs = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "ff5da644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"pin.bin\"\n",
    "model_pin.wv.save_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef18db6",
   "metadata": {},
   "source": [
    "Let's count the number of words in model_pin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "4a5174cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1703\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = model_pin.wv\n",
    "\n",
    "word_vectors.save(\"word2vec.wordvectors\")\n",
    "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
    "vector = wv['раскольников']\n",
    "\n",
    "print(len(model_pin.wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c42433",
   "metadata": {},
   "source": [
    "As we remember, Raskolnikov is a poor ex-law student who lives in a tiny garret, wearing old clothes due to lack of money. He is also not very kind to other people. Let's see if Raskolnikov would be close to vectors ['каморка', 'лохмотья'] substract ['добрый']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "88b5ac6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('раскольников', 0.9935407042503357),\n",
       " ('комната', 0.9933441877365112),\n",
       " ('глаз', 0.9911273121833801)]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pin.wv.most_similar(positive=[\"каморка\", \"лохмотья\"], negative=[\"добрый\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af070d",
   "metadata": {},
   "source": [
    "It is a succsess to find 'раскольников' on top, which is so close to vectors (almost 1). The result 'комната' also seems to be relevant to 'каморка'. We can also check how the model works for separate words like 'бедность'. I would suggest the following result as a relative success as the model defines 'нищета' as the second closest word to the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c9608e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('отсюда', 0.9975753426551819),\n",
       " ('нищета', 0.9974064230918884),\n",
       " ('гордый', 0.9967379570007324)]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pin.wv.most_similar(positive=[\"бедность\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd5d21",
   "metadata": {},
   "source": [
    "Next, let's see how 'раскольников' and 'топор' semanticaly close to each other. The cosine similarity of ~0,88 indicates that these words are almost similar to each other, which is a success, as we remember from the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "609e7356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88679916"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pin.wv.similarity(\"раскольников\", \"топор\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95900ad0",
   "metadata": {},
   "source": [
    "The following step shows that among words 'мысль', 'процентщица', 'топор', 'убивать', 'раскольников' the model indicates 'раскольников' as not matching one, which is quite unexpected (my expectation was 'мысль'). It seems like the model will output 'раскольников' every time this word is in the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "4f69894d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'раскольников'"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pin.wv.doesnt_match(\"мысль процентщица топор убивать раскольников\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9fa384",
   "metadata": {},
   "source": [
    "Then, let's visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "cce8bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "877bb440",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '1703' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-332-d103cd1a6508>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_pin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key '1703' not present\""
     ]
    }
   ],
   "source": [
    "words = list(model_pin.wv)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4a198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
